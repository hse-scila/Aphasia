{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:38:52.927655Z",
     "start_time": "2025-04-09T13:38:47.433449Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import librosa\n",
    "# from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# from models.swishnet import SwishNet\n",
    "\n",
    "from src.utils import AphasiaDatasetMFCC, AphasiaDatasetSpectrogram, AphasiaDatasetWaveform\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from torch_audiomentations import Compose, AddColoredNoise, PitchShift, Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360a4d28a7769207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:38:52.933076Z",
     "start_time": "2025-04-09T13:38:52.928625Z"
    }
   },
   "outputs": [],
   "source": [
    "# augmentations = Compose(\n",
    "#     transforms=[\n",
    "#         Gain(\n",
    "#             min_gain_in_db=-15.0,\n",
    "#             max_gain_in_db=5.0,\n",
    "#             sample_rate=8_000,\n",
    "#             p=0.5,\n",
    "#         ),\n",
    "#         AddColoredNoise(sample_rate=8_000, p=0.75),\n",
    "#         PitchShift(sample_rate=8_000, p=0.75),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bee62a27f3e0a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:38:52.985Z",
     "start_time": "2025-04-09T13:38:52.933709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's cpu time!!!\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AUDIO_LENGTH = 6_000\n",
    "SEQUENCE_LENGTH = 420\n",
    "MFCC = 128\n",
    "print(f\"It's {DEVICE} time!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d67c2b0e4a0b833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:38:52.995511Z",
     "start_time": "2025-04-09T13:38:52.985728Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4c43e57780095f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.587Z",
     "start_time": "2025-04-09T13:38:52.996513Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"train_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, mfcc=MFCC, n_mels=128, fft_size=512,\n",
    "                 hop_length=256, win_length=512, min_duration=10, max_duration=15, )\n",
    "test_dataset = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"val_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, mfcc=MFCC, n_mels=128, fft_size=512,\n",
    "                 hop_length=256, win_length=512, min_duration=10, max_duration=15)\n",
    "val_dataset = AphasiaDatasetMFCC(os.path.join(DATA_DIR, \"test_filenames.csv\"), VOICES_DIR, target_sample_rate=8_000, mfcc=MFCC, n_mels=128, fft_size=512,\n",
    "                 hop_length=256, win_length=512, min_duration=10, max_duration=15)\n",
    "\n",
    "# Балансировка классов для train\n",
    "train_labels = [label for _, label in train_dataset.data]\n",
    "class_counts = Counter(train_labels)\n",
    "if len(class_counts) < 2:\n",
    "    raise ValueError(\"Один из классов отсутствует в тренировочном наборе\")\n",
    "\n",
    "class_weights = {label: 1.0 / count for label, count in class_counts.items()}\n",
    "weights = [class_weights[label] for _, label in train_dataset.data]\n",
    "train_sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4f6c6b34049476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.590504Z",
     "start_time": "2025-04-09T13:41:51.587818Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    if not batch:\n",
    "        return torch.zeros(0), torch.zeros(0)\n",
    "    \n",
    "    seq, labels = zip(*batch)\n",
    "    # print(seq[1], labels)\n",
    "    max_len = max(s.shape[1] for s in seq)\n",
    "    # print(max_len)\n",
    "\n",
    "    # print(seq[0].shape)\n",
    "    padded = torch.zeros(len(seq), MFCC, SEQUENCE_LENGTH)\n",
    "    for i, s in enumerate(seq):\n",
    "        # print(s.shape)\n",
    "        padded[i, ..., :s.shape[-1]] = s[..., :SEQUENCE_LENGTH]\n",
    "    \n",
    "    return padded, torch.stack(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3138ee0a49d655eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.615954Z",
     "start_time": "2025-04-09T13:41:51.591133Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, collate_fn=pad_sequence, drop_last=True, num_workers=6)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=pad_sequence, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=pad_sequence, drop_last=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b611eaa77a4b849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.844738Z",
     "start_time": "2025-04-09T13:41:51.617870Z"
    }
   },
   "outputs": [],
   "source": [
    "t = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd4800867c6cfcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.849479Z",
     "start_time": "2025-04-09T13:41:51.845750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 420])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e531d4cb754794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.861650Z",
     "start_time": "2025-04-09T13:41:51.850076Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dl_train, dl_val, epochs=1, lr=0.001, device=\"cpu\"):\n",
    "      \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, threshold=1e-3)\n",
    "    # scheduler = step_scheduler(optimizer, 30) \n",
    "        \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training model\"):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        for features, target in dl_train:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features).squeeze()\n",
    "            # print(output.shape)\n",
    "            # train_acc.append(torch.tensor(torch.argmax(output, dim=1) == target).cpu().detach().numpy())\n",
    "            preds = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
    "            train_acc.append(accuracy_score(target.cpu().detach().numpy(), preds))\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.detach().item()\n",
    "        # print(f\"LR before: {optimizer.param_groups[0]['lr']}\")\n",
    "        # scheduler.step(epoch=epoch)\n",
    "        # print(f\"LR after: {optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        avg_train_acc = np.stack(train_acc, axis=0).mean()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, target in dl_val:\n",
    "                features, target = features.to(device), target.to(device)\n",
    "                \n",
    "                output = model(features).squeeze()\n",
    "                \n",
    "                preds = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
    "                val_acc.append(accuracy_score(target.cpu().detach().numpy(), preds))\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.detach().item()\n",
    "        \n",
    "        avg_val_acc = np.stack(val_acc, axis=0).mean()\n",
    "        avg_val_loss = total_val_loss / len(dl_val)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "        \n",
    "        if scheduler:\n",
    "            try:\n",
    "                scheduler.step()\n",
    "            except:\n",
    "                scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch}: train loss: {avg_train_loss:.3f}, train balanced acc: {avg_train_acc:.2f}, test loss: {avg_val_loss:.3f}, test balanced acc: {avg_val_acc:.2f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            \n",
    "    return model, train_loss_list, val_loss_list, train_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e49368b560693c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.888776Z",
     "start_time": "2025-04-09T13:41:51.862403Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MaskedConv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, mask=None, input_size=42):\n",
    "        super(MaskedConv1d, self).__init__()\n",
    "\n",
    "        # padding = max(0, (math.ceil((stride - 1) * in_channels + (kernel_size - 1) * dilation + 1 - stride) / stride - input_size))\n",
    "        # padding = (stride * (input_size - 1) + kernel_size - input_size) // 2\n",
    "        # padding = stride * (kernel_size - 1) * dilation // 2\n",
    "        # if kernel_size % 2 == 0:\n",
    "        #     padding -= 1\n",
    "        # padding = ((kernel_size - 1) * dilation - stride + 1)// 2\n",
    "        padding = math.ceil(((input_size - 1) * stride + 1 + dilation * (kernel_size - 1) - input_size) / 2)\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "\n",
    "        # if mask is None:\n",
    "        #     self.mask = torch.ones(kernel_size)\n",
    "        # else:\n",
    "        #     self.mask = mask.detach().clone()\n",
    "        # \n",
    "        # self.mask = nn.Parameter(self.mask, requires_grad=False)\n",
    "        # \n",
    "        # with torch.no_grad():\n",
    "        #     self.conv.weight = torch.nn.Parameter(self.mask * self.conv.weight)\n",
    "        # \n",
    "        # self.conv.weight.register_hook(lambda x: x * self.mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class GatedActivation(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GatedActivation, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_sigmoid = self.sigmoid(x)\n",
    "        x_tanh = self.tanh(x)\n",
    "        return x_sigmoid * x_tanh * x   # x * ???\n",
    "\n",
    "\n",
    "class GatedConv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, mask=None, input_size=42):\n",
    "        super(GatedConv1d, self).__init__()\n",
    "\n",
    "        self.conv = MaskedConv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, mask=mask,\n",
    "                                 stride=stride, dilation=dilation, input_size=input_size)\n",
    "\n",
    "        self.gate = GatedActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.gate(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwishNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=2, input_size=42, dropout_rate=0.1):\n",
    "        super(SwishNet, self).__init__()\n",
    "\n",
    "        kernel_size_3 = 3\n",
    "        mask_3 = torch.ones(kernel_size_3)\n",
    "        mask_3[kernel_size_3 // 2 + 1:] -= 1\n",
    "\n",
    "        kernel_size_6 = 5\n",
    "        mask_6 = torch.ones(kernel_size_6)\n",
    "        mask_6[kernel_size_6 // 2:] -= 1\n",
    "\n",
    "        self.gatedconv1_1 = GatedConv1d(in_channels=in_channels, out_channels=16, kernel_size=kernel_size_3, mask=mask_3, input_size=input_size)\n",
    "        self.gatedconv1_2 = GatedConv1d(in_channels=in_channels, out_channels=16, kernel_size=kernel_size_6, mask=mask_6, input_size=input_size)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.gatedconv2_1 = GatedConv1d(in_channels=32, out_channels=8, kernel_size=kernel_size_3, mask=mask_3, input_size=input_size)\n",
    "        self.gatedconv2_2 = GatedConv1d(in_channels=32, out_channels=8, kernel_size=kernel_size_6, mask=mask_6, input_size=input_size)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.batchnorm2 = nn.BatchNorm1d(16)\n",
    "        self.gatedconv3_1 = GatedConv1d(in_channels=16, out_channels=8, kernel_size=kernel_size_3, mask=mask_3, input_size=input_size)\n",
    "        self.gatedconv3_2 = GatedConv1d(in_channels=16, out_channels=8, kernel_size=kernel_size_6, mask=mask_6, input_size=input_size)\n",
    "\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.batchnorm3 = nn.BatchNorm1d(16)\n",
    "        self.gatedconv4 = GatedConv1d(in_channels=16, out_channels=16, kernel_size=kernel_size_3, mask=mask_3, stride=3, input_size=input_size)\n",
    "        \n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.batchnorm4 = nn.BatchNorm1d(16)\n",
    "        self.gatedconv5 = GatedConv1d(in_channels=16, out_channels=16, kernel_size=kernel_size_3, mask=mask_3, stride=2, input_size=input_size)\n",
    "\n",
    "        self.batchnorm5 = nn.BatchNorm1d(16)\n",
    "        self.dropout5 = nn.Dropout(dropout_rate)\n",
    "        self.gatedconv6 = GatedConv1d(in_channels=16, out_channels=16, kernel_size=kernel_size_3, mask=mask_3, stride=2, input_size=input_size)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.batchnorm6 = nn.BatchNorm1d(16)\n",
    "        self.gatedconv7 = GatedConv1d(in_channels=16, out_channels=16, kernel_size=kernel_size_3, mask=mask_3, stride=2, input_size=input_size) # 2\n",
    "\n",
    "        self.batchnorm7 = nn.BatchNorm1d(16)\n",
    "        self.dropout7 = nn.Dropout(dropout_rate)\n",
    "        self.gateconv8 = GatedConv1d(in_channels=16, out_channels=32, kernel_size=kernel_size_3, mask=mask_3, stride=1, input_size=input_size) # 2\n",
    "\n",
    "        # self.gateconv9 = GatedConv1d(in_channels=80, out_channels=out_channels, kernel_size=1, input_size=input_size)\n",
    "\n",
    "        self.batchnorm8 = nn.BatchNorm1d(32)\n",
    "        self.conv9 = nn.Conv1d(in_channels=80, out_channels=out_channels, kernel_size=1,)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.gatedconv1_1(x)\n",
    "        x1_2 = self.gatedconv1_2(x)\n",
    "\n",
    "        x1 = torch.cat([x1_1, x1_2], dim=1)\n",
    "\n",
    "        x1 = self.batchnorm1(x1)\n",
    "\n",
    "        x2_1 = self.gatedconv2_1(x1)\n",
    "        x2_2 = self.gatedconv2_2(x1)\n",
    "        x2 = torch.cat([x2_1, x2_2], dim=1)\n",
    "\n",
    "        x2 = self.batchnorm2(x2)\n",
    "\n",
    "        x3_1 = self.gatedconv3_1(x2)\n",
    "        x3_2 = self.gatedconv3_2(x2)\n",
    "        x3 = torch.cat([x3_1, x3_2], dim=1) + x2\n",
    "\n",
    "        x3 = self.batchnorm3(x3)\n",
    "        x3 = self.dropout3(x3)\n",
    "\n",
    "        x4 = self.gatedconv4(x3) # + x3\n",
    "\n",
    "        x4 += x3\n",
    "\n",
    "        x4 = self.batchnorm4(x4)\n",
    "\n",
    "        x5_cat = self.gatedconv5(x4)\n",
    "\n",
    "        x5 = x5_cat + x4\n",
    "\n",
    "        x5 = self.batchnorm5(x5)\n",
    "\n",
    "        x6_cat = self.gatedconv6(x5)\n",
    "        x6 = x6_cat + x5\n",
    "\n",
    "        x6 = self.batchnorm6(x6)\n",
    "        x6 = self.dropout6(x6)\n",
    "\n",
    "        x7_cat = self.gatedconv7(x6)\n",
    "\n",
    "        x7_cat = self.batchnorm7(x7_cat)\n",
    "\n",
    "        x8_cat = self.gateconv8(x7_cat)\n",
    "\n",
    "        x8_cat = self.batchnorm8(x8_cat)\n",
    "\n",
    "        x_cat = torch.cat([x5_cat, x6_cat, x7_cat, x8_cat], dim=1)\n",
    "\n",
    "        # x_cat = self.batchnorm8(x_cat)\n",
    "\n",
    "        x9 = self.conv9(x_cat)\n",
    "\n",
    "        x10 = self.global_pool(x9)\n",
    "\n",
    "        return self.softmax(x10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7163389fe66fd354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:51.913906Z",
     "start_time": "2025-04-09T13:41:51.889547Z"
    }
   },
   "outputs": [],
   "source": [
    "swishnet = SwishNet(MFCC, 2, input_size=SEQUENCE_LENGTH, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8765a788649bfd7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:57.743280Z",
     "start_time": "2025-04-09T13:41:51.915250Z"
    }
   },
   "outputs": [],
   "source": [
    "# swishnet, train_l, val_l, train_accuracy, val_accuracy = train_model(swishnet, train_dataloader, val_dataloader, epochs=6, lr=1e-3, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef6aad16f3f3ff30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:57.766131Z",
     "start_time": "2025-04-09T13:41:57.746882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = torch.load(os.path.join(os.getcwd(), \"checkpoints\", \"swishnet_chkp\", \"swishnet_0.911_best_.pt\"), weights_only=False)   # 6 эпох, а не 10\n",
    "\n",
    "swishnet = SwishNet(MFCC, 2, input_size=SEQUENCE_LENGTH, dropout_rate=0.5)\n",
    "\n",
    "swishnet.load_state_dict(st[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c97c1d1de8f512",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.71      0.77       102\n",
    "           1       0.96      0.98      0.97       730\n",
    "\n",
    "    accuracy                           0.95       832\n",
    "    macro avg       0.91      0.84      0.87       832\n",
    "    weighted avg    0.95      0.95      0.95       832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434e9d4fe12a95dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:58.392649Z",
     "start_time": "2025-04-09T13:41:58.381035Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'train_filenames.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ac69846f55cd166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:58.401499Z",
     "start_time": "2025-04-09T13:41:58.393492Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "VOICES_DIR = os.path.join(DATA_DIR, 'Voices_wav')\n",
    "APHASIA_DIR = os.path.join(VOICES_DIR, 'Aphasia')\n",
    "NORM_DIR = os.path.join(VOICES_DIR, 'Norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b75467190b97aa8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:41:58.413966Z",
     "start_time": "2025-04-09T13:41:58.402155Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model_for_each_participant(model, test_data):\n",
    "    model = model.to(\"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data[\"ID\"] = test_data[\"file_name\"].apply(\n",
    "        lambda x: str(x).split(\"-\")[0] + str(x).split(\"-\")[1])\n",
    "    test_data.head()\n",
    "    IDs = test_data[\"ID\"].unique()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for participant_id in tqdm(IDs):\n",
    "            participant_samples = test_data[test_data[\"ID\"] == participant_id]\n",
    "            preds = []\n",
    "            for ind, participant_sample in participant_samples.iterrows():\n",
    "\n",
    "                sgnl_path = participant_sample[\"file_name\"]\n",
    "\n",
    "                if participant_sample['label'] == 0:\n",
    "                    sgnl_path = os.path.join(NORM_DIR, sgnl_path)\n",
    "                else:\n",
    "                    sgnl_path = os.path.join(APHASIA_DIR, sgnl_path)\n",
    "                    \n",
    "                chunks = train_dataset.process_audio(sgnl_path)\n",
    "\n",
    "                padded = torch.zeros(len(chunks), MFCC, SEQUENCE_LENGTH)\n",
    "                for i, s in enumerate(chunks):\n",
    "                    padded[i, ..., :s.shape[-1]] = s[..., :SEQUENCE_LENGTH]\n",
    "                pred = model(torch.from_numpy(np.array(padded))).detach().numpy().squeeze(-1)\n",
    "\n",
    "                preds.append(pred)\n",
    "            labels = participant_samples[\"label\"]\n",
    "  \n",
    "            pred = np.concatenate(preds, axis=0).mean(axis=0).argmax(axis=-1)\n",
    "\n",
    "            all_preds.append(pred)\n",
    "        \n",
    "            all_labels.append(labels.values[0])\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3afe4281e46dfb0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:42:06.682193Z",
     "start_time": "2025-04-09T13:41:58.414790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/211 [00:00<?, ?it/s]/tmp/ipykernel_152321/1088031325.py:32: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  pred = model(torch.from_numpy(np.array(padded))).detach().numpy().squeeze(-1)\n",
      "100%|██████████| 211/211 [00:22<00:00,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97        60\n",
      "           1       0.98      0.99      0.99       151\n",
      "\n",
      "    accuracy                           0.98       211\n",
      "   macro avg       0.98      0.97      0.98       211\n",
      "weighted avg       0.98      0.98      0.98       211\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_for_each_participant(swishnet, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d2589d32412d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:42:06.689427Z",
     "start_time": "2025-04-09T13:42:06.687820Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphasia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
